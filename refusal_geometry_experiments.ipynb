{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b6a68ab0",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b33ea2",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r latent_adversarial_training/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdaa1cb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import functools\n",
        "import einops\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import gc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from jaxtyping import Float, Int\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login as hf_login\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4bf02d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def authenticate_hf():\n",
        "    token = os.environ.get(\"HF_TOKEN\")\n",
        "    if token:\n",
        "        hf_login(token=token)\n",
        "        print(\"Auth passed with HF_TOKEN \")\n",
        "    else:\n",
        "        try:\n",
        "            hf_login()\n",
        "            print(\"Auth passed via HF portal\")\n",
        "        except Exception as e:\n",
        "            print(f\"Auth failed: {e}\")\n",
        "            print(\"Run 'huggingface-cli login' or set HF_TOKEN\")\n",
        "            raise\n",
        "\n",
        "authenticate_hf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea09c39",
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "INFERENCE_DTYPE = torch.float16\n",
        "SEED = 3252\n",
        "\n",
        "LLAMA_2_7B_CHAT_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "PEFT_MODEL_PATH_LAT = \"nlpett/llama-2-7b-chat-hf-LAT-layer4-hh\"\n",
        "PEFT_MODEL_PATH_AT = \"nlpett/llama-2-7b-chat-hf-AT-hh\"\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "LLAMA_CHAT_TEMPLATE = \"\\n[INST]{prompt}[/INST]\\n\"\n",
        "\n",
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output dir: {OUTPUT_DIR.resolve()}\")\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b288b7e",
      "metadata": {},
      "source": [
        "### Load data & test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09effb60",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def get_harmful_instructions() -> Tuple[List[str], List[str]]:\n",
        "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
        "    response = requests.get(url)\n",
        "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
        "    instructions = dataset['goal'].tolist()\n",
        "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def get_harmless_instructions() -> Tuple[List[str], List[str]]:\n",
        "    hf_path = 'tatsu-lab/alpaca'\n",
        "    dataset = load_dataset(hf_path)\n",
        "    instructions = [\n",
        "        dataset['train'][i]['instruction'] \n",
        "        for i in range(len(dataset['train']))\n",
        "        if dataset['train'][i]['input'].strip() == '' # filter instructions wihtout inputs\n",
        "    ]\n",
        "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
        "    return train, test\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "harmful_train, harmful_test = get_harmful_instructions()\n",
        "harmless_train, harmless_test = get_harmless_instructions()\n",
        "\n",
        "print(f\"Harmful instructions: {len(harmful_train)} train, {len(harmful_test)} test\")\n",
        "print(f\"Harmless instructions: {len(harmless_train)} train, {len(harmless_test)} test\")\n",
        "\n",
        "print(\"\\nHarmful examples:\")\n",
        "for i in range(3):\n",
        "    print(f\"  {i+1}. {harmful_train[i][:80]}...\")\n",
        "print(\"\\nHarmless examples:\")\n",
        "for i in range(3):\n",
        "    print(f\"  {i+1}. {harmless_train[i][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bbecdc",
      "metadata": {},
      "source": [
        "### Load models & get activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5142311",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_base_model() -> HookedTransformer:\n",
        "    print(\"Loading baseline model...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLAMA_2_7B_CHAT_PATH,\n",
        "        torch_dtype=INFERENCE_DTYPE,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    \n",
        "    hooked_model = HookedTransformer.from_pretrained_no_processing(\n",
        "        LLAMA_2_7B_CHAT_PATH,\n",
        "        hf_model=base_model,\n",
        "        dtype=INFERENCE_DTYPE,\n",
        "        device=DEVICE,\n",
        "        default_padding_side='left',\n",
        "    )\n",
        "    hooked_model.tokenizer.padding_side = 'left'\n",
        "    hooked_model.tokenizer.pad_token = \"[PAD]\"\n",
        "    \n",
        "    del base_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return hooked_model\n",
        "\n",
        "def load_peft_model(peft_path: str, model_name: str = \"LAT\") -> HookedTransformer:\n",
        "    print(f\"Loading {model_name} model from {peft_path}...\")\n",
        "    \n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLAMA_2_7B_CHAT_PATH,\n",
        "        torch_dtype=INFERENCE_DTYPE,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    # merge PEFT adapter    \n",
        "    peft_model = PeftModel.from_pretrained(base_model, peft_path)\n",
        "    peft_model = peft_model.to(INFERENCE_DTYPE)\n",
        "    \n",
        "    try:\n",
        "        merged_model = peft_model.merge_and_unload()\n",
        "        print(f\"Successfully merged {model_name} adapter.\")\n",
        "    except AttributeError:\n",
        "        print(f\"{model_name} doesn't support merging, using as-is.\")\n",
        "        merged_model = peft_model\n",
        "    \n",
        "    merged_model.eval()\n",
        "    \n",
        "    hooked_model = HookedTransformer.from_pretrained_no_processing(\n",
        "        LLAMA_2_7B_CHAT_PATH,\n",
        "        hf_model=merged_model,\n",
        "        dtype=INFERENCE_DTYPE,\n",
        "        device=DEVICE,\n",
        "        default_padding_side='left',\n",
        "    )\n",
        "    hooked_model.tokenizer.padding_side = 'left'\n",
        "    hooked_model.tokenizer.pad_token = \"[PAD]\"\n",
        "    \n",
        "    del merged_model, peft_model, base_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return hooked_model\n",
        "\n",
        "# Load both models into a dictionary\n",
        "# Can fit both on a 80GB A100 ~14GB each in fp16\n",
        "\n",
        "models = {}\n",
        "\n",
        "print(\"Loading baseline...\")\n",
        "models['Baseline'] = load_base_model()\n",
        "print(f\"  Baseline loaded: {models['Baseline'].cfg.n_layers} layers, {models['Baseline'].cfg.d_model} hidden dim\")\n",
        "\n",
        "print(\"\\nLoading LAT-adapter...\")\n",
        "models['LAT'] = load_peft_model(PEFT_MODEL_PATH_LAT, \"LAT\")\n",
        "print(f\"  LAT loaded: {models['LAT'].cfg.n_layers} layers, {models['LAT'].cfg.d_model} hidden dim\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8f8b47",
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    reserved = torch.cuda.memory_reserved() / 1e9\n",
        "    print(f\"\\nGPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93d48cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_instructions(\n",
        "    tokenizer: AutoTokenizer,\n",
        "    instructions: List[str]\n",
        ") -> Int[Tensor, 'batch_size seq_len']:\n",
        "    \"\"\"Tokenize instructions using Llama-2 chat template.\"\"\"\n",
        "    prompts = [LLAMA_CHAT_TEMPLATE.format(prompt=instruction) for instruction in instructions]\n",
        "    return tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids\n",
        "\n",
        "def get_activations(\n",
        "    model: HookedTransformer,\n",
        "    instructions: List[str],\n",
        "    layer: int,\n",
        "    batch_size: int = 32,\n",
        "    position: int = -1  # Last token by default\n",
        ") -> torch.Tensor:\n",
        "    # Get activations at layer n for a given instruction\n",
        "    # run_with_cache for residual stream activations as we go\n",
        "\n",
        "    tokenize_fn = functools.partial(tokenize_instructions, tokenizer=model.tokenizer)\n",
        "    activations = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(instructions), batch_size), desc=f\"Layer {layer}\"):\n",
        "        batch = instructions[i:i+batch_size]\n",
        "        toks = tokenize_fn(instructions=batch).to(DEVICE)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, cache = model.run_with_cache( # fw pass and cache intermediate residual stream activations\n",
        "                toks,\n",
        "                names_filter=lambda name: f'blocks.{layer}.hook_resid_pre' in name\n",
        "            )\n",
        "        activations.append(cache[f'blocks.{layer}.hook_resid_pre'][:, position, :]) \n",
        "    \n",
        "    return torch.cat(activations, dim=0)\n",
        "\n",
        "def get_activations_all_layers(\n",
        "    model: HookedTransformer,\n",
        "    instructions: List[str],\n",
        "    layers: List[int],\n",
        "    batch_size: int = 16,\n",
        "    position: int = -1\n",
        ") -> Dict[int, torch.Tensor]:\n",
        "    # Get activations at multiple layers\n",
        "    tokenize_fn = functools.partial(tokenize_instructions, tokenizer=model.tokenizer)\n",
        "    activations = {l: [] for l in layers}\n",
        "    \n",
        "    for i in tqdm(range(0, len(instructions), batch_size), desc=\"Extracting activations\"):\n",
        "        batch = instructions[i:i+batch_size]\n",
        "        toks = tokenize_fn(instructions=batch).to(DEVICE)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, cache = model.run_with_cache(\n",
        "                toks,\n",
        "                names_filter=lambda name: 'resid_pre' in name\n",
        "            )\n",
        "        \n",
        "        for l in layers:\n",
        "            activations[l].append(cache['resid_pre', l][:, position, :])\n",
        "    \n",
        "    return {l: torch.cat(acts, dim=0) for l, acts in activations.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8b2649",
      "metadata": {},
      "source": [
        "### Compute refusal vectors\n",
        "\n",
        "The difference of mean activations between harmful and harmless prompts. L14 has highest efficacy of refusal direction ablation aross all model variants - see 14.3 in https://arxiv.org/pdf/2504.18872"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd7fa94",
      "metadata": {},
      "outputs": [],
      "source": [
        "# global refusal vector computation confiog\n",
        "N_SAMPLES = 100  # samples to compute vector\n",
        "TARGET_LAYER = 14  # target activation layer\n",
        "\n",
        "harmful_acts = {}   # {model_name: tensor}\n",
        "harmless_acts = {}  # {model_name: tensor}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"     activations for {model_name} at layer {TARGET_LAYER}...\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    harmful_acts[model_name] = get_activations(model, harmful_train[:N_SAMPLES], TARGET_LAYER, batch_size=16)\n",
        "    harmless_acts[model_name] = get_activations(model, harmless_train[:N_SAMPLES], TARGET_LAYER, batch_size=16)\n",
        "    \n",
        "    print(f\"Harmful shape: {harmful_acts[model_name].shape}\")\n",
        "    print(f\"Harmless shape: {harmless_acts[model_name].shape}\")\n",
        "\n",
        "refusal_dirs = {}\n",
        "\n",
        "for model_name in models.keys():\n",
        "    harmful_mean = harmful_acts[model_name].mean(dim=0)\n",
        "    harmless_mean = harmless_acts[model_name].mean(dim=0)\n",
        "    \n",
        "    refusal_dir = harmful_mean - harmless_mean\n",
        "    refusal_dirs[model_name] = {\n",
        "        'raw': refusal_dir,\n",
        "        'normalized': refusal_dir / refusal_dir.norm(),\n",
        "        'norm': refusal_dir.norm().item()\n",
        "    }\n",
        "    print(f\"{model_name}: refusal direction norm = {refusal_dir.norm().item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b3dfe6",
      "metadata": {},
      "source": [
        "### Analysis of selected layer and refusal vector composition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f4e3e2",
      "metadata": {},
      "source": [
        "#### Cosine similarity of baseline and LAT refusal vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8e8f51",
      "metadata": {},
      "outputs": [],
      "source": [
        "# cosine similarity of baseline and LAT directions\n",
        "cos_sim = torch.cosine_similarity(\n",
        "    refusal_dirs['Baseline']['normalized'].unsqueeze(0),\n",
        "    refusal_dirs['LAT']['normalized'].unsqueeze(0)\n",
        ").item()\n",
        "print(f\"\\nCosine similarity between Baseline and LAT refusal directions: {cos_sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe9c425",
      "metadata": {},
      "source": [
        "#### PCA of layer specific (14) activations in baseline and LAT-adapted models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c108c644",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "for idx, (model_name, ax) in enumerate(zip(models.keys(), axes)):\n",
        "    h_acts = harmful_acts[model_name]\n",
        "    hl_acts = harmless_acts[model_name]\n",
        "    \n",
        "    all_acts = torch.cat([hl_acts, h_acts], dim=0).cpu().float().numpy()\n",
        "    \n",
        "    pca = PCA(n_components=2)\n",
        "    acts_pca = pca.fit_transform(all_acts)\n",
        "    \n",
        "    n_harmless = len(hl_acts)\n",
        "    ax.scatter(acts_pca[:n_harmless, 0], acts_pca[:n_harmless, 1], \n",
        "               c='blue', alpha=0.6, label='Harmless', s=50)\n",
        "    ax.scatter(acts_pca[n_harmless:, 0], acts_pca[n_harmless:, 1], \n",
        "               c='red', alpha=0.6, label='Harmful', s=50)\n",
        "    \n",
        "    harmless_pca_mean = acts_pca[:n_harmless].mean(axis=0)\n",
        "    harmful_pca_mean = acts_pca[n_harmless:].mean(axis=0)\n",
        "    ax.scatter(*harmless_pca_mean, c='blue', s=200, marker='X', edgecolors='black', linewidths=2)\n",
        "    ax.scatter(*harmful_pca_mean, c='red', s=200, marker='X', edgecolors='black', linewidths=2)\n",
        "    \n",
        "    ax.annotate('', xy=harmful_pca_mean, xytext=harmless_pca_mean,\n",
        "                arrowprops=dict(arrowstyle='->', color='green', lw=3))\n",
        "    \n",
        "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
        "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
        "    ax.set_title(f'{model_name} Model\\nLayer {TARGET_LAYER}')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('PCA of Activations: Baseline vs LAT', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / f'pca_layer{TARGET_LAYER}_comparison.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6167faa7",
      "metadata": {},
      "source": [
        "#### Plot SVD concentration across layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048d1536",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot SVD concentration comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "colors = {'Baseline': 'blue', 'LAT': 'orange'}\n",
        "markers = {'Baseline': 'o', 'LAT': 's'}\n",
        "\n",
        "# Left plot: Top-2 variance by layer for both models\n",
        "for model_name in models.keys():\n",
        "    layers = list(svd_results[model_name].keys())\n",
        "    top2_var = [svd_results[model_name][l]['top_k_variance'] * 100 for l in layers]\n",
        "    axes[0].plot(layers, top2_var, f'{markers[model_name]}-', \n",
        "                 linewidth=2, markersize=8, color=colors[model_name], label=model_name)\n",
        "\n",
        "axes[0].axhline(y=44, color='gray', linestyle=':', alpha=0.7, label='Abbas ref: Baseline (44%)')\n",
        "axes[0].axhline(y=75, color='gray', linestyle='--', alpha=0.7, label='Abbas ref: LAT (75%)')\n",
        "axes[0].set_xlabel('Layer', fontsize=12)\n",
        "axes[0].set_ylabel('Variance in Top-2 Components (%)', fontsize=12)\n",
        "axes[0].set_title('SVD Concentration by Layer', fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_ylim(0, 100)\n",
        "\n",
        "# Right plot: Singular value spectrum at layer 14 for both models\n",
        "width = 0.35\n",
        "x = np.arange(20)\n",
        "for i, model_name in enumerate(models.keys()):\n",
        "    var_exp = svd_results[model_name][TARGET_LAYER]['variance_explained'][:20] * 100\n",
        "    axes[1].bar(x + i*width - width/2, var_exp, width, \n",
        "                label=model_name, color=colors[model_name], alpha=0.7, edgecolor='black')\n",
        "\n",
        "axes[1].set_xlabel('Component Index', fontsize=12)\n",
        "axes[1].set_ylabel('Variance Explained (%)', fontsize=12)\n",
        "base_top2 = svd_results['Baseline'][TARGET_LAYER]['top_k_variance'] * 100\n",
        "lat_top2 = svd_results['LAT'][TARGET_LAYER]['top_k_variance'] * 100\n",
        "axes[1].set_title(f'Singular Value Spectrum at Layer {TARGET_LAYER}\\nBaseline: {base_top2:.1f}% | LAT: {lat_top2:.1f}%', fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'svd_concentration_comparison.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd4c637",
      "metadata": {},
      "outputs": [],
      "source": [
        "LAYERS_TO_ANALYZE = [0, 4, 8, 12, 14, 16, 20, 24, 28, 31]\n",
        "\n",
        "svd_results = {model_name: {} for model_name in models.keys()}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"SVD analysis for {model_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    for layer in tqdm(LAYERS_TO_ANALYZE, desc=f\"{model_name}\"):\n",
        "        harmful_layer = get_activations(model, harmful_train[:N_SAMPLES], layer, batch_size=16)\n",
        "        harmless_layer = get_activations(model, harmless_train[:N_SAMPLES], layer, batch_size=16)\n",
        "        \n",
        "        directions = compute_difference_directions(harmful_layer, harmless_layer)\n",
        "        \n",
        "        svd_results[model_name][layer] = compute_svd_concentration(directions, top_k=2)\n",
        "        \n",
        "        del harmful_layer, harmless_layer, directions\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SVD Concentration Comparison (variance in top-2 components)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Layer':<8} {'Baseline':<12} {'LAT':<12} {'Î”':<10}\")\n",
        "print(\"-\"*42)\n",
        "for layer in LAYERS_TO_ANALYZE:\n",
        "    base = svd_results['Baseline'][layer]['top_k_variance'] * 100\n",
        "    lat = svd_results['LAT'][layer]['top_k_variance'] * 100\n",
        "    delta = lat - base\n",
        "    print(f\"{layer:<8} {base:<12.1f} {lat:<12.1f} {delta:+.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f2a5d5",
      "metadata": {},
      "source": [
        "---\n",
        "## IGNORE FOR NOW: GRAM MATRIX AND INTERFERENCE ANALYSIS\n",
        "The Gram matrix G = D @ D^T captures pairwise similarities between difference directions. Off-diagonal elements measure \"interference\" - how much different prompts' refusal directions overlap.\n",
        "\n",
        "**Key metrics:**\n",
        "- Mean off-diagonal: Average |G_ij| for i â‰  j (lower = less interference)\n",
        "- Interference Frobenius: ||G - diag(G)||_F (total interference magnitude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033dee1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gram_matrix(directions: torch.Tensor, normalize: bool = True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute Gram matrix G = D @ D^T\n",
        "    If normalize=True, computes cosine similarity matrix instead.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        norms = directions.norm(dim=1, keepdim=True)\n",
        "        norms = torch.clamp(norms, min=1e-8)\n",
        "        directions_normed = directions / norms\n",
        "        gram = directions_normed @ directions_normed.T\n",
        "    else:\n",
        "        gram = directions @ directions.T\n",
        "    return gram\n",
        "\n",
        "\n",
        "def compute_interference_metrics(gram: torch.Tensor) -> dict:\n",
        "    \"\"\"\n",
        "    Compute interference metrics from Gram matrix.\n",
        "    \"\"\"\n",
        "    n = gram.shape[0]\n",
        "    \n",
        "    # Extract off-diagonal elements\n",
        "    mask = ~torch.eye(n, dtype=torch.bool, device=gram.device)\n",
        "    off_diag = gram[mask]\n",
        "    \n",
        "    # Mean off-diagonal magnitude\n",
        "    mean_off_diagonal = off_diag.abs().mean().item()\n",
        "    \n",
        "    # Interference Frobenius norm: ||G - diag(G)||_F\n",
        "    diag_matrix = torch.diag(torch.diag(gram))\n",
        "    interference_frobenius = (gram - diag_matrix).norm('fro').item()\n",
        "    \n",
        "    # Standard deviation of off-diagonal\n",
        "    off_diagonal_std = off_diag.std().item()\n",
        "    \n",
        "    return {\n",
        "        'mean_off_diagonal': mean_off_diagonal,\n",
        "        'interference_frobenius': interference_frobenius,\n",
        "        'off_diagonal_std': off_diagonal_std,\n",
        "        'off_diagonal_values': off_diag.cpu().numpy()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "590f2a9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Gram matrix and interference for BOTH models at layer 14\n",
        "interference_results = {}\n",
        "\n",
        "for model_name in models.keys():\n",
        "    # Compute difference directions\n",
        "    directions = compute_difference_directions(harmful_acts[model_name], harmless_acts[model_name])\n",
        "    \n",
        "    # Compute normalized Gram matrix (cosine similarities)\n",
        "    gram = compute_gram_matrix(directions, normalize=True)\n",
        "    \n",
        "    # Compute interference metrics\n",
        "    interference = compute_interference_metrics(gram)\n",
        "    interference['gram'] = gram\n",
        "    interference['directions'] = directions\n",
        "    interference_results[model_name] = interference\n",
        "\n",
        "# Print comparison\n",
        "print(\"=\"*60)\n",
        "print(f\"Interference Metrics at Layer {TARGET_LAYER}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<30} {'Baseline':<15} {'LAT':<15}\")\n",
        "print(\"-\"*60)\n",
        "for metric in ['mean_off_diagonal', 'interference_frobenius', 'off_diagonal_std']:\n",
        "    base = interference_results['Baseline'][metric]\n",
        "    lat = interference_results['LAT'][metric]\n",
        "    print(f\"{metric:<30} {base:<15.4f} {lat:<15.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10305fd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Gram matrices for BOTH models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "for idx, model_name in enumerate(models.keys()):\n",
        "    gram = interference_results[model_name]['gram']\n",
        "    off_diag = interference_results[model_name]['off_diagonal_values']\n",
        "    \n",
        "    # Gram matrix heatmap\n",
        "    im = axes[idx, 0].imshow(gram.cpu().numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "    axes[idx, 0].set_title(f'{model_name}: Cosine Similarity Matrix', fontsize=12)\n",
        "    axes[idx, 0].set_xlabel('Prompt Pair Index')\n",
        "    axes[idx, 0].set_ylabel('Prompt Pair Index')\n",
        "    plt.colorbar(im, ax=axes[idx, 0], label='Cosine Similarity')\n",
        "    \n",
        "    # Histogram of off-diagonal values\n",
        "    axes[idx, 1].hist(off_diag, bins=50, edgecolor='black', alpha=0.7, \n",
        "                      color='blue' if model_name == 'Baseline' else 'orange')\n",
        "    axes[idx, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
        "    axes[idx, 1].axvline(x=off_diag.mean(), color='green', linestyle='--', linewidth=2, \n",
        "                         label=f'Mean: {off_diag.mean():.3f}')\n",
        "    mean_abs = interference_results[model_name]['mean_off_diagonal']\n",
        "    axes[idx, 1].set_title(f'{model_name}: Off-Diagonal Distribution\\nMean |sim|: {mean_abs:.3f}', fontsize=12)\n",
        "    axes[idx, 1].set_xlabel('Cosine Similarity')\n",
        "    axes[idx, 1].set_ylabel('Count')\n",
        "    axes[idx, 1].legend()\n",
        "    axes[idx, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle(f'Gram Matrix Analysis at Layer {TARGET_LAYER}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / f'gram_matrix_layer{TARGET_LAYER}_comparison.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747922d2",
      "metadata": {},
      "source": [
        "---\n",
        "## IGNORE FOR NOW: Baseline Model Measurements\n",
        "\n",
        "This establishes baseline measurements for the key metrics. Next steps:\n",
        "1. Load LAT model and repeat measurements\n",
        "2. Compare concentration and interference between Baseline vs LAT\n",
        "3. Test whether higher concentration correlates with lower interference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6865dcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary comparison\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPARISON SUMMARY: Baseline vs LAT\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nLayer {TARGET_LAYER} (refusal-critical layer):\")\n",
        "print(f\"\\n{'Metric':<35} {'Baseline':<15} {'LAT':<15} {'Change':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# SVD Concentration\n",
        "base_svd = svd_results['Baseline'][TARGET_LAYER]['top_k_variance'] * 100\n",
        "lat_svd = svd_results['LAT'][TARGET_LAYER]['top_k_variance'] * 100\n",
        "print(f\"{'SVD Top-2 Variance (%)':<35} {base_svd:<15.1f} {lat_svd:<15.1f} {lat_svd - base_svd:+.1f}\")\n",
        "\n",
        "# Interference metrics\n",
        "base_interf = interference_results['Baseline']['mean_off_diagonal']\n",
        "lat_interf = interference_results['LAT']['mean_off_diagonal']\n",
        "print(f\"{'Mean Off-Diagonal |cos sim|':<35} {base_interf:<15.4f} {lat_interf:<15.4f} {lat_interf - base_interf:+.4f}\")\n",
        "\n",
        "base_frob = interference_results['Baseline']['interference_frobenius']\n",
        "lat_frob = interference_results['LAT']['interference_frobenius']\n",
        "print(f\"{'Interference Frobenius Norm':<35} {base_frob:<15.4f} {lat_frob:<15.4f} {lat_frob - base_frob:+.4f}\")\n",
        "\n",
        "print(f\"\\nReference values from Abbas et al.:\")\n",
        "print(f\"  Baseline concentration: ~44%\")\n",
        "print(f\"  LAT concentration:      ~75%\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Key finding\n",
        "print(\"\\nðŸ”‘ KEY FINDING:\")\n",
        "if lat_svd > base_svd and lat_interf < base_interf:\n",
        "    print(\"   LAT shows HIGHER concentration AND LOWER interference\")\n",
        "    print(\"   â†’ Supports hypothesis that concentration reduces interference\")\n",
        "elif lat_svd > base_svd and lat_interf >= base_interf:\n",
        "    print(\"   LAT shows HIGHER concentration BUT similar/higher interference\")\n",
        "    print(\"   â†’ Concentration and interference may be independent phenomena\")\n",
        "else:\n",
        "    print(\"   Results need further investigation\")\n",
        "\n",
        "# Save results\n",
        "all_results = {}\n",
        "for model_name in models.keys():\n",
        "    all_results[model_name] = {\n",
        "        'layer': TARGET_LAYER,\n",
        "        'n_samples': N_SAMPLES,\n",
        "        'svd_top2_variance': svd_results[model_name][TARGET_LAYER]['top_k_variance'],\n",
        "        'mean_off_diagonal': interference_results[model_name]['mean_off_diagonal'],\n",
        "        'interference_frobenius': interference_results[model_name]['interference_frobenius'],\n",
        "    }\n",
        "\n",
        "with open(OUTPUT_DIR / 'comparison_results.json', 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "print(f\"\\nResults saved to {OUTPUT_DIR / 'comparison_results.json'}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
