{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cb9c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: transformers>=4.36.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.57.3)\n",
      "Requirement already satisfied: transformer_lens>=1.14.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.16.1)\n",
      "Requirement already satisfied: peft>=0.7.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: accelerate>=0.25.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.12.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.19.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.36.0)\n",
      "Requirement already satisfied: datasets>=2.14.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (4.4.2)\n",
      "Requirement already satisfied: einops>=0.7.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.8.1)\n",
      "Requirement already satisfied: jaxtyping>=0.2.25 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.3.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.3.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.10.8)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.7.2)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (2.32.5)\n",
      "Requirement already satisfied: colorama>=0.4.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: Pillow>=10.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (12.1.0)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (1.2.1)\n",
      "Requirement already satisfied: bitsandbytes>=0.41.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (0.49.0)\n",
      "Requirement already satisfied: sae-lens>=4.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.27.1)\n",
      "Requirement already satisfied: wandb>=0.15.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (0.23.1)\n",
      "Requirement already satisfied: zstandard>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (6.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (0.22.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.36.0->-r requirements.txt (line 3)) (2025.11.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (0.2.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (0.0.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (14.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (0.14.1)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (4.4.4)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (0.0.5)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens>=1.14.0->-r requirements.txt (line 4)) (0.0.3)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from peft>=0.7.0->-r requirements.txt (line 5)) (5.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (2024.3.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (0.70.18)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (0.28.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=2.14.0->-r requirements.txt (line 8)) (22.0.0)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from jaxtyping>=0.2.25->-r requirements.txt (line 10)) (0.1.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 11)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 11)) (2025.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 11)) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 12)) (4.29.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 12)) (3.3.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 13)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 13)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/lib/python3/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 13)) (1.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.31.0->-r requirements.txt (line 15)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.31.0->-r requirements.txt (line 15)) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.31.0->-r requirements.txt (line 15)) (2020.6.20)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.31.0->-r requirements.txt (line 15)) (3.4.4)\n",
      "Requirement already satisfied: tenacity>=9.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sae-lens>=4.0.0->-r requirements.txt (line 20)) (9.1.2)\n",
      "Requirement already satisfied: babe<0.0.8,>=0.0.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.0.7)\n",
      "Requirement already satisfied: plotly>=5.19.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sae-lens>=4.0.0->-r requirements.txt (line 20)) (6.5.0)\n",
      "Requirement already satisfied: simple-parsing<0.2.0,>=0.1.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.1.7)\n",
      "Requirement already satisfied: plotly-express>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.4.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from sae-lens>=4.0.0->-r requirements.txt (line 20)) (3.9.2)\n",
      "Requirement already satisfied: pydantic<3 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.15.0->-r requirements.txt (line 21)) (2.12.5)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/lib/python3/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 21)) (4.21.12)\n",
      "Requirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 21)) (2.5.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.15.0->-r requirements.txt (line 21)) (3.1.46)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/lib/python3/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 21)) (8.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.15.0->-r requirements.txt (line 21)) (2.48.0)\n",
      "Requirement already satisfied: graze in /home/ubuntu/.local/lib/python3.10/site-packages (from babe<0.0.8,>=0.0.7->sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.1.39)\n",
      "Requirement already satisfied: py2store in /home/ubuntu/.local/lib/python3.10/site-packages (from babe<0.0.8,>=0.0.7->sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.1.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/.local/lib/python3.10/site-packages (from fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (3.13.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 21)) (4.0.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.14.0->-r requirements.txt (line 8)) (1.0.9)\n",
      "Requirement already satisfied: anyio in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.14.0->-r requirements.txt (line 8)) (4.12.0)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ubuntu/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.14.0->-r requirements.txt (line 8)) (0.16.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from plotly>=5.19.0->sae-lens>=4.0.0->-r requirements.txt (line 20)) (2.14.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from plotly-express>=0.4.1->sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.14.6)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from plotly-express>=0.4.1->sae-lens>=4.0.0->-r requirements.txt (line 20)) (1.0.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3->wandb>=0.15.0->-r requirements.txt (line 21)) (2.41.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3->wandb>=0.15.0->-r requirements.txt (line 21)) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3->wandb>=0.15.0->-r requirements.txt (line 21)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens>=1.14.0->-r requirements.txt (line 4)) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens>=1.14.0->-r requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/ubuntu/.local/lib/python3.10/site-packages (from simple-parsing<0.2.0,>=0.1.6->sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.17.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (1.22.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (1.8.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (0.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface_hub>=0.19.0->-r requirements.txt (line 7)) (6.7.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 21)) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens>=1.14.0->-r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.14.0->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: dol in /home/ubuntu/.local/lib/python3.10/site-packages (from graze->babe<0.0.8,>=0.0.7->sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.3.38)\n",
      "Requirement already satisfied: importlib-resources in /home/ubuntu/.local/lib/python3.10/site-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens>=4.0.0->-r requirements.txt (line 20)) (6.5.2)\n",
      "Requirement already satisfied: config2py in /home/ubuntu/.local/lib/python3.10/site-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.1.45)\n",
      "Requirement already satisfied: i2 in /home/ubuntu/.local/lib/python3.10/site-packages (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens>=4.0.0->-r requirements.txt (line 20)) (0.1.63)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a877b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2026-01-03 12:07:45.883198: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-03 12:07:45.894961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767442065.909504  154619 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767442065.914077  154619 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767442065.925323  154619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767442065.925339  154619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767442065.925341  154619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767442065.925343  154619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-03 12:07:45.928884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhal2k\u001b[0m (\u001b[33mhal2k-n-a\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login as hf_login\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from sae_lens import (\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    LanguageModelSAETrainingRunner,\n",
    "    CacheActivationsRunnerConfig,\n",
    "    CacheActivationsRunner,\n",
    "    BatchTopKTrainingSAEConfig,\n",
    "    LoggingConfig,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "hf_login(token=os.environ.get('HF_TOKEN'))\n",
    "wandb.login(key=os.environ.get('WANDB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d72164",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "LAT_ADAPTER = \"nlpett/llama-2-7b-chat-hf-LAT-layer4-hh\"\n",
    "OUTPUT_DIR = \"./sae_outputs\"\n",
    "CACHE_DIR = \"./cached_activations\"\n",
    "WANDB_PROJECT = \"lat-interference-analysis\"\n",
    "\n",
    "# SAE params\n",
    "D_IN = 4096\n",
    "D_EXP = 16\n",
    "D_SAE = D_EXP * D_IN\n",
    "K = 64                          # TopK sparsity\n",
    "TRAINING_TOKENS = 100_000_000   # 100M\n",
    "LAYER = 14\n",
    "HOOK = f\"blocks.{LAYER}.hook_resid_post\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13823781",
   "metadata": {},
   "source": [
    "#### Merge LAT adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee82a21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing merged model at ./sae_outputs/llama2-lat-merged\n"
     ]
    }
   ],
   "source": [
    "merged_path = f\"{OUTPUT_DIR}/llama2-lat-merged\"\n",
    "if not os.path.exists(merged_path):\n",
    "    base_hf = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16)\n",
    "    peft_model = PeftModel.from_pretrained(base_hf, LAT_ADAPTER)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_path)\n",
    "    del base_hf, peft_model, merged_model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Merged model saved to {merged_path}\")\n",
    "else:\n",
    "    print(f\"Using existing merged model at {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4cf22",
   "metadata": {},
   "source": [
    "#### Train baseline SAE (direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d1e23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'monology/pile-uncopyrighted' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sae_lens/training/activations_store.py:324: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://decoderesearch.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/latent_adversarial_refusal_geometry/wandb/run-20260103_013047-l0t94iae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/l0t94iae' target=\"_blank\">baseline-layer14-sae</a></strong> to <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/l0t94iae' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/l0t94iae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24400| mse_loss: 126.56625 | auxiliary_reconstruction_loss: 1.00317: 100%|█████████▉| 99942400/100000000 [2:35:24<00:05, 10718.57it/s]   \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_samples</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▅▆▇█▇▇▇▇▇▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▆▆▆▅▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▂▁▁▁▂▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▆▆▅▅▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▅▁▆▇▆▆▅▇▇▇██▆▇█▄▄▆██▇▇▇█▇▇▇▇▇▇▇█▇█▇▇▇█▇█</td></tr><tr><td>metrics/explained_variance_legacy</td><td>▁▄▄▄▄▇▇▇▇▇▇▇▇████▇▇█████▇█▇██████████▇██</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>▅▇█▇▇▆▆▅▃▄▃▄▂▃▃▃▃▃▃▂▂▃▂▂▂▁▃▂▂▂▃▃▂▂▃▂▂▃▂▂</td></tr><tr><td>metrics/l0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>▂▁▆▇▇███████</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0001</td></tr><tr><td>details/n_training_samples</td><td>99983360</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0.86578</td></tr><tr><td>losses/mse_loss</td><td>124.67271</td></tr><tr><td>losses/overall_loss</td><td>125.5385</td></tr><tr><td>metrics/explained_variance</td><td>0.99505</td></tr><tr><td>metrics/explained_variance_legacy</td><td>0.75112</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>0.07803</td></tr><tr><td>metrics/l0</td><td>64</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.54223</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline-layer14-sae</strong> at: <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/l0t94iae' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/l0t94iae</a><br> View project at: <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260103_013047-l0t94iae/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline SAE saved to ./sae_outputs/baseline_sae_final\n"
     ]
    }
   ],
   "source": [
    "baseline_sae_cfg = LanguageModelSAERunnerConfig(\n",
    "    # Direct model access\n",
    "    model_name=BASE_MODEL,\n",
    "    model_class_name=\"HookedTransformer\",\n",
    "    hook_name=HOOK,\n",
    "    \n",
    "    # Dataset\n",
    "    dataset_path=\"monology/pile-uncopyrighted\",\n",
    "    streaming=True,\n",
    "    context_size=512,\n",
    "    \n",
    "    # SAE architecture\n",
    "    sae=BatchTopKTrainingSAEConfig(\n",
    "        d_in=D_IN,\n",
    "        d_sae=D_SAE,\n",
    "        k=K,\n",
    "    ),\n",
    "    \n",
    "    # Training params\n",
    "    lr=1e-4,\n",
    "    train_batch_size_tokens=4096,\n",
    "    training_tokens=TRAINING_TOKENS,\n",
    "    n_batches_in_buffer=32,       # Reduced from 128 to fit in GPU memory\n",
    "    store_batch_size_prompts=16,  # Reduced from 32 \n",
    "    \n",
    "    # Precision\n",
    "    dtype=\"float32\",\n",
    "    autocast=True,\n",
    "    autocast_lm=True,\n",
    "    \n",
    "    # Logging\n",
    "    logger=LoggingConfig(\n",
    "        log_to_wandb=True,\n",
    "        wandb_project=WANDB_PROJECT,\n",
    "        run_name=\"baseline-layer14-sae\",\n",
    "    ),\n",
    "    \n",
    "    checkpoint_path=f\"{OUTPUT_DIR}/baseline_checkpoints\",\n",
    "    n_checkpoints=3,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "baseline_sae = LanguageModelSAETrainingRunner(baseline_sae_cfg).run()\n",
    "baseline_sae.save_model(f\"{OUTPUT_DIR}/baseline_sae_final\")\n",
    "print(f\"Baseline SAE saved to {OUTPUT_DIR}/baseline_sae_final\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d758bd",
   "metadata": {},
   "source": [
    "#### Train LAT SAE (direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0480a25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged LAT model into HookedTransformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You just passed in a model which will override the one specified in your configuration: meta-llama/Llama-2-7b-chat-hf. As a consequence this run will not be reproducible via configuration alone.\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'monology/pile-uncopyrighted' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAT model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sae_lens/training/activations_store.py:324: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://decoderesearch.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/latent_adversarial_refusal_geometry/wandb/run-20260103_120831-q5psuu9y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/q5psuu9y' target=\"_blank\">lat-layer14-sae</a></strong> to <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/q5psuu9y' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/q5psuu9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24400| mse_loss: 111.07465 | auxiliary_reconstruction_loss: 1.52489: 100%|█████████▉| 99942400/100000000 [2:33:45<00:05, 10833.17it/s]   \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_samples</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁█▇█▇▇▅▂▄▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▇▆▆▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▅▅▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▄▆▄▅▆▇▇█▇▇█▇▁▇██▇▇██▇█▇█▇▇█▇█▇██████▇███</td></tr><tr><td>metrics/explained_variance_legacy</td><td>▁▂▅▅▆▆▇▇▇▇▇▇▇█▇█▇▇▇▇█▇██████████████████</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>▅▇█▆▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>▁▁▆▇▇▇██████</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0001</td></tr><tr><td>details/n_training_samples</td><td>99983360</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>1.48027</td></tr><tr><td>losses/mse_loss</td><td>108.02708</td></tr><tr><td>losses/overall_loss</td><td>109.50735</td></tr><tr><td>metrics/explained_variance</td><td>0.99356</td></tr><tr><td>metrics/explained_variance_legacy</td><td>0.83571</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>0.06251</td></tr><tr><td>metrics/l0</td><td>64</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.54946</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lat-layer14-sae</strong> at: <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/q5psuu9y' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis/runs/q5psuu9y</a><br> View project at: <a href='https://wandb.ai/hal2k-n-a/lat-interference-analysis' target=\"_blank\">https://wandb.ai/hal2k-n-a/lat-interference-analysis</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260103_120831-q5psuu9y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAT SAE saved to ./sae_outputs/lat_sae_final\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Load the merged LAT model into TransformerLens manually\n",
    "print(\"Loading merged LAT model into HookedTransformer...\")\n",
    "lat_model = HookedTransformer.from_pretrained(\n",
    "    BASE_MODEL,  # Use base model architecture/config\n",
    "    hf_model=AutoModelForCausalLM.from_pretrained(merged_path, torch_dtype=torch.float32),\n",
    "    device=\"cuda\",\n",
    "    center_writing_weights=False,\n",
    ")\n",
    "print(\"LAT model loaded successfully!\")\n",
    "\n",
    "lat_sae_cfg = LanguageModelSAERunnerConfig(\n",
    "    # Model config (name for reference, actual model passed via override_model)\n",
    "    model_name=BASE_MODEL,  # Keep original name for config compatibility\n",
    "    model_class_name=\"HookedTransformer\",\n",
    "    hook_name=HOOK,\n",
    "    \n",
    "    # Dataset (identical)\n",
    "    dataset_path=\"monology/pile-uncopyrighted\",\n",
    "    streaming=True,\n",
    "    context_size=512,\n",
    "    \n",
    "    # SAE architecture (identical)\n",
    "    sae=BatchTopKTrainingSAEConfig(\n",
    "        d_in=D_IN,\n",
    "        d_sae=D_SAE,\n",
    "        k=K,\n",
    "    ),\n",
    "    \n",
    "    # Training params (identical)\n",
    "    lr=1e-4,\n",
    "    train_batch_size_tokens=4096,\n",
    "    training_tokens=TRAINING_TOKENS,\n",
    "    n_batches_in_buffer=32,       # Reduced from 128 to fit in GPU memory\n",
    "    store_batch_size_prompts=16,  # Reduced from 32 to fit in GPU memory\n",
    "    \n",
    "    # Precision (identical)\n",
    "    dtype=\"float32\",\n",
    "    autocast=True,\n",
    "    autocast_lm=True,\n",
    "    \n",
    "    # Logging\n",
    "    logger=LoggingConfig(\n",
    "        log_to_wandb=True,\n",
    "        wandb_project=WANDB_PROJECT,\n",
    "        run_name=\"lat-layer14-sae\",\n",
    "    ),\n",
    "    \n",
    "    checkpoint_path=f\"{OUTPUT_DIR}/lat_checkpoints\",\n",
    "    n_checkpoints=3,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Pass the pre-loaded LAT model via override_model\n",
    "lat_sae = LanguageModelSAETrainingRunner(lat_sae_cfg, override_model=lat_model).run()\n",
    "lat_sae.save_model(f\"{OUTPUT_DIR}/lat_sae_final\")\n",
    "print(f\"LAT SAE saved to {OUTPUT_DIR}/lat_sae_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb7193",
   "metadata": {},
   "source": [
    "#### Compute interference matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c58798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAEs from disk if not already in memory\n",
    "from sae_lens import SAE\n",
    "\n",
    "if 'baseline_sae' not in dir() or baseline_sae is None:\n",
    "    print(\"Loading baseline SAE from disk...\")\n",
    "    baseline_sae = SAE.load_from_pretrained(f\"{OUTPUT_DIR}/baseline_sae_final\")\n",
    "\n",
    "if 'lat_sae' not in dir() or lat_sae is None:\n",
    "    print(\"Loading LAT SAE from disk...\")\n",
    "    lat_sae = SAE.load_from_pretrained(f\"{OUTPUT_DIR}/lat_sae_final\")\n",
    "\n",
    "# Extract decoder weights: shape is (d_sae, d_in) = (65536, 4096)\n",
    "W_baseline = baseline_sae.W_dec.detach().cpu().numpy()\n",
    "W_lat = lat_sae.W_dec.detach().cpu().numpy()\n",
    "\n",
    "# Normalize each feature vector (row) to unit norm\n",
    "W_baseline_norm = W_baseline / (np.linalg.norm(W_baseline, axis=1, keepdims=True) + 1e-8)\n",
    "W_lat_norm = W_lat / (np.linalg.norm(W_lat, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "# Gram matrices: G[i,j] = cos(feature_i, feature_j)\n",
    "print(\"Computing Gram matrices (this may take a minute)...\")\n",
    "G_baseline = W_baseline_norm @ W_baseline_norm.T\n",
    "G_lat = W_lat_norm @ W_lat_norm.T\n",
    "\n",
    "# Extract off-diagonal elements (all pairwise interferences)\n",
    "n = G_baseline.shape[0]\n",
    "mask = ~np.eye(n, dtype=bool)\n",
    "off_diag_baseline = np.abs(G_baseline[mask])\n",
    "off_diag_lat = np.abs(G_lat[mask])\n",
    "\n",
    "# Also check dead features (features with near-zero norm)\n",
    "baseline_norms = np.linalg.norm(W_baseline, axis=1)\n",
    "lat_norms = np.linalg.norm(W_lat, axis=1)\n",
    "baseline_dead = np.mean(baseline_norms < 1e-6)\n",
    "lat_dead = np.mean(lat_norms < 1e-6)\n",
    "\n",
    "# Compute all metrics\n",
    "results = {\n",
    "    \"config\": {\n",
    "        \"d_in\": D_IN,\n",
    "        \"d_sae\": D_SAE,\n",
    "        \"k\": K,\n",
    "        \"training_tokens\": TRAINING_TOKENS,\n",
    "        \"layer\": LAYER,\n",
    "    },\n",
    "    \"baseline\": {\n",
    "        \"mean_interference\": float(np.mean(off_diag_baseline)),\n",
    "        \"median_interference\": float(np.median(off_diag_baseline)),\n",
    "        \"max_interference\": float(np.max(off_diag_baseline)),\n",
    "        \"p95_interference\": float(np.percentile(off_diag_baseline, 95)),\n",
    "        \"p99_interference\": float(np.percentile(off_diag_baseline, 99)),\n",
    "        \"frac_below_0.1\": float(np.mean(off_diag_baseline < 0.1)),\n",
    "        \"frac_below_0.05\": float(np.mean(off_diag_baseline < 0.05)),\n",
    "        \"dead_features_frac\": float(baseline_dead),\n",
    "    },\n",
    "    \"lat\": {\n",
    "        \"mean_interference\": float(np.mean(off_diag_lat)),\n",
    "        \"median_interference\": float(np.median(off_diag_lat)),\n",
    "        \"max_interference\": float(np.max(off_diag_lat)),\n",
    "        \"p95_interference\": float(np.percentile(off_diag_lat, 95)),\n",
    "        \"p99_interference\": float(np.percentile(off_diag_lat, 99)),\n",
    "        \"frac_below_0.1\": float(np.mean(off_diag_lat < 0.1)),\n",
    "        \"frac_below_0.05\": float(np.mean(off_diag_lat < 0.05)),\n",
    "        \"dead_features_frac\": float(lat_dead),\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'Metric':<25} {'Baseline':>12} {'LAT':>12} {'Ratio':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in [\"mean_interference\", \"median_interference\", \"max_interference\", \n",
    "               \"p95_interference\", \"p99_interference\", \"frac_below_0.1\", \n",
    "               \"frac_below_0.05\", \"dead_features_frac\"]:\n",
    "    b = results[\"baseline\"][metric]\n",
    "    l = results[\"lat\"][metric]\n",
    "    ratio = l / b if b > 1e-10 else float('inf')\n",
    "    print(f\"{metric:<25} {b:>12.6f} {l:>12.6f} {ratio:>10.3f}\")\n",
    "\n",
    "# Key interpretation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ratio = results[\"lat\"][\"mean_interference\"] / results[\"baseline\"][\"mean_interference\"]\n",
    "print(f\"\\nMean interference ratio (LAT/Baseline): {ratio:.3f}\")\n",
    "\n",
    "if ratio < 0.7:\n",
    "    print(\">>> LAT shows REDUCED interference (supports Gorton hypothesis)\")\n",
    "elif ratio > 1.3:\n",
    "    print(\">>> LAT shows INCREASED interference (contradicts Gorton hypothesis)\")\n",
    "else:\n",
    "    print(\">>> No significant difference in interference\")\n",
    "\n",
    "print(f\"\\nGorton et al. benchmark: robust models have ~0.5x the interference of non-robust\")\n",
    "\n",
    "if abs(baseline_dead - lat_dead) > 0.05:\n",
    "    print(f\"\\nWARNING: Dead feature rates differ significantly ({baseline_dead:.1%} vs {lat_dead:.1%})\")\n",
    "    print(\"         This may confound the interference comparison\")\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/interference_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/interference_results.json\")\n",
    "print(f\"W&B dashboard: https://wandb.ai/hal2k-n-a/{WANDB_PROJECT}\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
