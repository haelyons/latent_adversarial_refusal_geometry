{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing how quantization affects the observed SVD concentration of refusal directions.\n",
        "Precisions: 4-bit, 8-bit, 16-bit (fp16), 32-bit (fp32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import svd\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login as hf_login\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "hf_login(token=os.environ.get('HF_TOKEN'))\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "LLAMA_PATH = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "LAT_PATH = 'nlpett/llama-2-7b-chat-hf-LAT-layer4-hh'\n",
        "AT_PATH = 'nlpett/llama-2-7b-chat-hf-AT-hh'\n",
        "TEMPLATE = '\\n[INST]{prompt}[/INST]\\n'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# precision configs - each returns (quantization_config or None, dtype, name)\n",
        "def get_precision_config(bits):\n",
        "    if bits == 4:\n",
        "        return BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type='nf4'\n",
        "        ), torch.float16, '4-bit'\n",
        "    elif bits == 8:\n",
        "        return BitsAndBytesConfig(load_in_8bit=True), torch.float16, '8-bit'\n",
        "    elif bits == 16:\n",
        "        return None, torch.float16, 'fp16'\n",
        "    elif bits == 32:\n",
        "        return None, torch.float32, 'fp32'\n",
        "    else:\n",
        "        raise ValueError(f'Unknown precision: {bits}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clear_gpu():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "def load_model(peft_path, quant_config, dtype):\n",
        "    kwargs = {'torch_dtype': dtype, 'device_map': 'auto'}\n",
        "    if quant_config:\n",
        "        kwargs['quantization_config'] = quant_config\n",
        "    \n",
        "    base = AutoModelForCausalLM.from_pretrained(LLAMA_PATH, **kwargs)\n",
        "    \n",
        "    if peft_path:\n",
        "        peft = PeftModel.from_pretrained(base, peft_path)\n",
        "        if quant_config is None:\n",
        "            peft = peft.to(dtype)\n",
        "        merged = peft.merge_and_unload()\n",
        "        merged.eval()\n",
        "        hf_model = merged\n",
        "        del peft\n",
        "    else:\n",
        "        hf_model = base\n",
        "    \n",
        "    hooked = HookedTransformer.from_pretrained_no_processing(\n",
        "        LLAMA_PATH, hf_model=hf_model, dtype=dtype, device=DEVICE, default_padding_side='left'\n",
        "    )\n",
        "    hooked.tokenizer.padding_side = 'left'\n",
        "    hooked.tokenizer.pad_token = '[PAD]'\n",
        "    \n",
        "    del base\n",
        "    if peft_path:\n",
        "        del hf_model\n",
        "    clear_gpu()\n",
        "    return hooked\n",
        "\n",
        "def unload_model(model):\n",
        "    del model\n",
        "    clear_gpu()\n",
        "    print(f'GPU memory after cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_acts(model, instructions, layer=14, batch_size=16):\n",
        "    acts = []\n",
        "    for i in tqdm(range(0, len(instructions), batch_size), desc=f'L{layer}'):\n",
        "        batch = instructions[i:i+batch_size]\n",
        "        prompts = [TEMPLATE.format(prompt=p) for p in batch]\n",
        "        toks = model.tokenizer(prompts, padding=True, return_tensors='pt').input_ids.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            _, cache = model.run_with_cache(toks, names_filter=lambda n: f'blocks.{layer}.hook_resid_pre' in n)\n",
        "        acts.append(cache[f'blocks.{layer}.hook_resid_pre'][:, -1, :].cpu())  # move to CPU immediately\n",
        "        del cache\n",
        "    return torch.cat(acts, dim=0)\n",
        "\n",
        "def svd_variance(diffs):\n",
        "    _, s, _ = svd(diffs.float().numpy(), full_matrices=False)\n",
        "    v = (s**2) / np.sum(s**2)\n",
        "    return {'top1': v[0], 'top2': sum(v[:2]), 'top10': sum(v[:10])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
        "harmful_train, _ = train_test_split(\n",
        "    pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')))['goal'].tolist(), \n",
        "    test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dataset = load_dataset('tatsu-lab/alpaca')\n",
        "harmless_all = [dataset['train'][i]['instruction'] for i in range(len(dataset['train'])) \n",
        "                if dataset['train'][i]['input'].strip() == '']\n",
        "harmless_train, _ = train_test_split(harmless_all, test_size=0.2, random_state=42)\n",
        "\n",
        "N = 416\n",
        "print(f'Using {N} samples each')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# main experiment: iterate precision -> model -> compute SVD\n",
        "PRECISIONS = [4, 8, 16, 32]\n",
        "MODELS = [('BASE', None), ('AT', AT_PATH), ('LAT', LAT_PATH)]\n",
        "\n",
        "results = {bits: {} for bits in PRECISIONS}\n",
        "\n",
        "for bits in PRECISIONS:\n",
        "    quant_config, dtype, prec_name = get_precision_config(bits)\n",
        "    print(f'\\n{\"=\"*50}\\n{prec_name.upper()} PRECISION\\n{\"=\"*50}')\n",
        "    \n",
        "    for model_name, peft_path in MODELS:\n",
        "        print(f'\\n--- {model_name} ---')\n",
        "        \n",
        "        try:\n",
        "            model = load_model(peft_path, quant_config, dtype)\n",
        "            print(f'Loaded. GPU: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
        "            \n",
        "            harmful_acts = get_acts(model, harmful_train[:N])\n",
        "            harmless_acts = get_acts(model, harmless_train[:N])\n",
        "            diffs = harmful_acts - harmless_acts\n",
        "            \n",
        "            results[bits][model_name] = svd_variance(diffs)\n",
        "            print(f\"{model_name} @ {prec_name}: top-1={results[bits][model_name]['top1']*100:.1f}%\")\n",
        "            \n",
        "            del harmful_acts, harmless_acts, diffs\n",
        "            unload_model(model)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f'ERROR: {e}')\n",
        "            results[bits][model_name] = None\n",
        "            clear_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'Precision':<12} {'BASE':>10} {'AT':>10} {'LAT':>10}\")\n",
        "for bits in PRECISIONS:\n",
        "    _, _, prec_name = get_precision_config(bits)\n",
        "    row = f\"{prec_name:<12}\"\n",
        "    for model_name in ['BASE', 'AT', 'LAT']:\n",
        "        if results[bits].get(model_name):\n",
        "            row += f\"{results[bits][model_name]['top1']*100:>10.1f}\"\n",
        "        else:\n",
        "            row += f\"{'N/A':>10}\"\n",
        "    print(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "prec_labels = ['4-bit', '8-bit', 'fp16', 'fp32']\n",
        "x = np.arange(len(PRECISIONS))\n",
        "width = 0.25\n",
        "\n",
        "colors = {'BASE': 'tab:blue', 'AT': 'tab:orange', 'LAT': 'tab:red'}\n",
        "\n",
        "for i, model_name in enumerate(['BASE', 'AT', 'LAT']):\n",
        "    vals = []\n",
        "    for bits in PRECISIONS:\n",
        "        if results[bits].get(model_name):\n",
        "            vals.append(results[bits][model_name]['top1'] * 100)\n",
        "        else:\n",
        "            vals.append(0)\n",
        "    ax.bar(x + (i - 1) * width, vals, width, label=model_name, color=colors[model_name], alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Precision')\n",
        "ax.set_ylabel('Top-1 SVD Explained Variance (%)')\n",
        "ax.set_title('Refusal Direction Concentration vs Model Precision')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(prec_labels)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "ax.set_ylim(0, 105)\n",
        "\n",
        "#ax.axhline(y=51, color='tab:red', linestyle='--', alpha=0.3, label='Paper LAT (~51%)')\n",
        "#ax.axhline(y=43, color='tab:orange', linestyle='--', alpha=0.3, label='Paper AT (~43%)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# line plot version - concentration trend\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for model_name in ['BASE', 'AT', 'LAT']:\n",
        "    vals = []\n",
        "    for bits in PRECISIONS:\n",
        "        if results[bits].get(model_name):\n",
        "            vals.append(results[bits][model_name]['top1'] * 100)\n",
        "        else:\n",
        "            vals.append(np.nan)\n",
        "    ax.plot(prec_labels, vals, 'o-', label=model_name, color=colors[model_name], linewidth=2, markersize=8)\n",
        "\n",
        "ax.set_xlabel('Precision')\n",
        "ax.set_ylabel('Top-1 SVD Explained Variance (%)')\n",
        "ax.set_title('Quantisation effects on refusal direction SVD concentration in top component')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim(0, 105)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('precision_analysis_line.png', dpi=150)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
